{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ac2c75-a7c2-409b-a308-605e022868a9",
   "metadata": {},
   "source": [
    "Part 1: Understanding Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "593bb1a0-7b02-4f98-bcbc-527542c02938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhile building and training neural networks, it is crucial to initialize the weights appropriately to ensure a model\\nwith high accuracy. If the weights are not correctly initialized, it may give rise to the Vanishing Gradient problem \\nor the Exploding Gradient problem.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 1 :\n",
    "\"\"\"\n",
    "While building and training neural networks, it is crucial to initialize the weights appropriately to ensure a model\n",
    "with high accuracy. If the weights are not correctly initialized, it may give rise to the Vanishing Gradient problem \n",
    "or the Exploding Gradient problem.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b34edc-7325-4630-97b5-e2ce7aa7e366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOne of the main reasons for the slow convergence and the suboptimal generalization results of MLP \\n(Multilayer Perceptrons) based on gradient descent training is the lack of a proper initialization of the weights \\nto be adjusted. Even sophisticated learning procedures are not able to compensate for bad initial values of weights, \\nwhile good initial guess leads to fast convergence and or better generalization capability even with simple \\ngradient-based error minimization techniques. Although initial weight space in MLPs seems so critical there is no \\nstudy so far of its properties with regards to which regions lead to solutions or failures concerning generalization\\nand convergence in real world problems. There exist only some preliminary studies for toy problems, like XOR. \\nA data mining approach, based on Self Organizing Feature Maps (SOM), is involved in this paper to demonstrate that a \\ncomplete analysis of the MLP weight space is possible. This is the main novelty of this paper. The conclusions drawn \\nfrom this novel application of SOM algorithm in MLP analysis extend significantly previous preliminary results in the \\nliterature. MLP initialization procedures are overviewed along with all conclusions so far drawn in the literature and \\nan extensive experimental study on more representative tasks, using our data mining approach, reveals important initial\\nweight space properties of MLPs, extending previous knowledge and literature results.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 2 :\n",
    "\"\"\"\n",
    "One of the main reasons for the slow convergence and the suboptimal generalization results of MLP \n",
    "(Multilayer Perceptrons) based on gradient descent training is the lack of a proper initialization of the weights \n",
    "to be adjusted. Even sophisticated learning procedures are not able to compensate for bad initial values of weights, \n",
    "while good initial guess leads to fast convergence and or better generalization capability even with simple \n",
    "gradient-based error minimization techniques. Although initial weight space in MLPs seems so critical there is no \n",
    "study so far of its properties with regards to which regions lead to solutions or failures concerning generalization\n",
    "and convergence in real world problems. There exist only some preliminary studies for toy problems, like XOR. \n",
    "A data mining approach, based on Self Organizing Feature Maps (SOM), is involved in this paper to demonstrate that a \n",
    "complete analysis of the MLP weight space is possible. This is the main novelty of this paper. The conclusions drawn \n",
    "from this novel application of SOM algorithm in MLP analysis extend significantly previous preliminary results in the \n",
    "literature. MLP initialization procedures are overviewed along with all conclusions so far drawn in the literature and \n",
    "an extensive experimental study on more representative tasks, using our data mining approach, reveals important initial\n",
    "weight space properties of MLPs, extending previous knowledge and literature results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1552dae-6351-4c6a-88a2-ffa9abd9990f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during\\nthe course of a forward pass through a deep neural network. If either occurs, loss gradients will either be too\\nlarge or too small to flow backwards beneficially, and the network will take longer to converge, if it is even \\nable to do so at all.\\n\\nMatrix multiplication is the essential math operation of a neural network. In deep neural nets with several layers,\\none forward pass simply entails performing consecutive matrix multiplications at each layer, between that layer’s \\ninputs and weight matrix. The product of this multiplication at one layer becomes the inputs of the subsequent layer,\\nand so on and so forth.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 3 :\n",
    "\"\"\"\n",
    "The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during\n",
    "the course of a forward pass through a deep neural network. If either occurs, loss gradients will either be too\n",
    "large or too small to flow backwards beneficially, and the network will take longer to converge, if it is even \n",
    "able to do so at all.\n",
    "\n",
    "Matrix multiplication is the essential math operation of a neural network. In deep neural nets with several layers,\n",
    "one forward pass simply entails performing consecutive matrix multiplications at each layer, between that layer’s \n",
    "inputs and weight matrix. The product of this multiplication at one layer becomes the inputs of the subsequent layer,\n",
    "and so on and so forth.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab8c45-f45b-4f61-bfff-b9dd4df89792",
   "metadata": {},
   "source": [
    "Part 2: Weight Initialization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdcccffb-1abf-4a5c-9689-371b420ec23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nZero initialization causes the neuron to memorize the same functions almost in each iteration. Random initialization\\nis a better choice to break the symmetry. However, initializing weight with much high or low value can \\nresult in slower optimization.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 4 :\n",
    "\"\"\"\n",
    "Zero initialization causes the neuron to memorize the same functions almost in each iteration. Random initialization\n",
    "is a better choice to break the symmetry. However, initializing weight with much high or low value can \n",
    "result in slower optimization.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1241be5e-902e-49e2-b0e3-2550c1f3d675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRandom Initialization for neural networks aids in the symmetry-breaking process and improves accuracy.\\nThe weights are randomly initialized in this manner, very close to zero. \\nAs a result, symmetry is broken, and each neuron no longer performs the same computation.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 5 :\n",
    "\"\"\"\n",
    "Random Initialization for neural networks aids in the symmetry-breaking process and improves accuracy.\n",
    "The weights are randomly initialized in this manner, very close to zero. \n",
    "As a result, symmetry is broken, and each neuron no longer performs the same computation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40a8ace-4617-4515-8b94-5adf27777b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nXavier Glorot's initialization is one of the most widely used methods for initializing weight matrices in neural \\nnetworks. While in practice, it is straightforward to utilize in your deep learning setup, reflecting upon the \\nmathematical reasoning behind this standard initialization technique can prove most beneficial.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 6 :\n",
    "'''\n",
    "Xavier Glorot's initialization is one of the most widely used methods for initializing weight matrices in neural \n",
    "networks. While in practice, it is straightforward to utilize in your deep learning setup, reflecting upon the \n",
    "mathematical reasoning behind this standard initialization technique can prove most beneficial.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1532c8-ca1c-4ea3-8bc7-7bab82eaedf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn summary, the main difference for machine learning practitioners is the following: He initialization works better\\nfor layers with ReLu activation. Xavier initialization works better for layers with sigmoid activation.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 7 :\n",
    "\"\"\"\n",
    "In summary, the main difference for machine learning practitioners is the following: He initialization works better\n",
    "for layers with ReLu activation. Xavier initialization works better for layers with sigmoid activation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83353476-5eb9-4f38-9826-cf7cfed7b44a",
   "metadata": {},
   "source": [
    "Part 3: Applying Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6addce80-882b-4684-8281-acec3dcc5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q No. 8 :\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22042379-1317-4a86-9a27-f453d83885e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Initialization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    " \n",
    "initializer = tf.keras.initializers.Zeros()\n",
    "layer = tf.keras.layers.Dense(\n",
    "  3, kernel_initializer=initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae7d05e6-8af5-46ee-80e0-df9e1213603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Normal Distribution\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    " \n",
    "initializer = tf.keras.initializers.RandomNormal(\n",
    "  mean=0., stddev=1.)\n",
    "layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d72101b3-502e-46dc-8856-396f728279d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Uniform Initialization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    " \n",
    "initializer = tf.keras.initializers.RandomUniform(\n",
    "  minval=0.,maxval=1.)\n",
    "layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fa9c92c-a39f-4d25-9b40-8006c29715ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier/Glorot Uniform Initialization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    " \n",
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77e98f77-8d57-48c8-89dc-696a93f1c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normailzed Xavier/Glorot Uniform Initialization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    " \n",
    "initializer = tf.keras.initializers.GlorotNormal()\n",
    "layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8717dfd3-db0a-42c1-b676-603257724248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# He Uniform Initialization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    " \n",
    "initializer = tf.keras.initializers.HeUniform()\n",
    "layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34620d37-01b2-48fd-b352-003948a455a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# He Normal Initialization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    " \n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b3cea67-70f2-4f8f-bb6d-c8af7d55d9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            (None, 2)                 6         \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15 (60.00 Byte)\n",
      "Trainable params: 15 (60.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import python libraries required in this example:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "# Use numpy arrays to store inputs (x) and outputs (y):\n",
    "x = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([[0], [1], [1], [0]]) \n",
    "\n",
    "# Define the network model and its arguments. \n",
    "# Set the number of neurons/nodes for each layer:\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_shape=(2,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid')) \n",
    "model.add(layer)\n",
    "\n",
    "# Compile the model and calculate its accuracy:\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy']) \n",
    "\n",
    "# Print a summary of the Keras model:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c675f599-a586-4698-a8b7-e40f5405defb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe simplest way to initialize weights and biases is to set those to small uniform random values which works\\nwell for neural networks with a single hidden layer. But, when number of hidden layers is more than one, \\nthen you can use a good initialization scheme like “Glorot (also known as Xavier) Initialization”.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 9 :\n",
    "'''\n",
    "The simplest way to initialize weights and biases is to set those to small uniform random values which works\n",
    "well for neural networks with a single hidden layer. But, when number of hidden layers is more than one, \n",
    "then you can use a good initialization scheme like “Glorot (also known as Xavier) Initialization”.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
